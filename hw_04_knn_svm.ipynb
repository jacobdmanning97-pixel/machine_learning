{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: k-Nearest Neighbors and Support Vector Machines\n",
    "\n",
    "**Course:** MATH 8710: Introduction to Machine Learning I  \n",
    "**Topics Covered:** k-NN Classification, Support Vector Machines, Convex Optimization, Kernels  \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This assignment explores two fundamental machine learning algorithms: k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs). You will implement k-NN from scratch, apply SVMs with different kernels, and analyze the theoretical foundations of convex optimization that make SVMs work.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement k-NN classification and understand distance metrics\n",
    "- Apply SVMs with different kernels and tune hyperparameters\n",
    "- Understand convex optimization theory and KKT conditions\n",
    "- Compare algorithm performance on real datasets\n",
    "\n",
    "**Total Points: 100**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure PyTorch\n",
    "torch.manual_seed(42)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'lines.linewidth': 2\n",
    "})\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Environment configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 1: k-Nearest Neighbors Implementation (50 points)\n",
    "\n",
    "In this problem, you will implement k-NN classification from scratch and analyze its behavior with different distance metrics and values of k.\n",
    "\n",
    "## Part A: Implementation (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    \"\"\"\n",
    "    k-Nearest Neighbors classifier implementation from scratch.\n",
    "    \n",
    "    This implementation supports multiple distance metrics:\n",
    "    - 'euclidean': L2 norm (default)\n",
    "    - 'manhattan': L1 norm\n",
    "    - 'chebyshev': L∞ norm (maximum coordinate difference)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=5, distance_metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Initialize the k-NN classifier.\n",
    "        \n",
    "        Args:\n",
    "            k (int): Number of neighbors to consider\n",
    "            distance_metric (str): Distance metric to use\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the k-NN model by storing the training data.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Training features, shape (n_samples, n_features)\n",
    "            y (np.ndarray): Training labels, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        # STUDENT CODE START (4 points)\n",
    "        # TODO: Store the training data\n",
    "        pass\n",
    "        # STUDENT CODE END\n",
    "    \n",
    "    def compute_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute distance between two points using the specified metric.\n",
    "        \n",
    "        Args:\n",
    "            x1 (np.ndarray): First point, shape (n_features,)\n",
    "            x2 (np.ndarray): Second point, shape (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            float: Distance between x1 and x2\n",
    "        \"\"\"\n",
    "        # STUDENT CODE START (12 points - ~4 points per metric)\n",
    "        # TODO: Implement three distance metrics\n",
    "        \n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # Euclidean distance: sqrt(sum((x1 - x2)^2))\n",
    "            pass\n",
    "            \n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            # Manhattan distance: sum(|x1 - x2|)\n",
    "            pass\n",
    "            \n",
    "        elif self.distance_metric == 'chebyshev':\n",
    "            # Chebyshev distance: max(|x1 - x2|)\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "        # STUDENT CODE END\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class for a single test point.\n",
    "        \n",
    "        Args:\n",
    "            x (np.ndarray): Test point, shape (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            int: Predicted class label\n",
    "        \"\"\"\n",
    "        # STUDENT CODE START (12 points)\n",
    "        # TODO: Implement the k-NN prediction algorithm:\n",
    "        # 1. Compute distances to all training points\n",
    "        # 2. Find the k nearest neighbors\n",
    "        # 3. Return the majority class among these neighbors\n",
    "        pass\n",
    "        # STUDENT CODE END\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for multiple test points.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Test features, shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Predicted class labels, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        # STUDENT CODE START (2 points)\n",
    "        # TODO: Apply predict_single to each test point\n",
    "        pass\n",
    "        # STUDENT CODE END\n",
    "\n",
    "print(\"✓ KNNClassifier class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Testing and Analysis (20 points)\n",
    "\n",
    "Test your k-NN implementation on a synthetic dataset and compare different distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                          n_informative=2, n_clusters_per_class=1,\n",
    "                          random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features (important for k-NN!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE START (13 points)\n",
    "# TODO: Test your k-NN implementation with k=5 and all three distance metrics\n",
    "# For each metric:\n",
    "# 1. Create a KNNClassifier instance\n",
    "# 2. Fit it on the training data\n",
    "# 3. Make predictions on the test data\n",
    "# 4. Compute and print the accuracy\n",
    "\n",
    "distance_metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "print(\"k-NN Performance with Different Distance Metrics (k=5):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# STUDENT CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (7 points):** Based on your results above, which distance metric performed best? Why might different distance metrics give different results? In what scenarios might you prefer Manhattan distance over Euclidean distance?\n",
    "\n",
    "<!-- STUDENT ANSWER START -->\n",
    "**Your answer here:**\n",
    "\n",
    "*(Expected elements: Discussion of metric properties, sensitivity to feature scales, when Manhattan might be preferred (e.g., grid-like features, robustness to outliers), geometric interpretation)*\n",
    "<!-- STUDENT ANSWER END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 2: Support Vector Machines with Kernels (50 points)\n",
    "\n",
    "In this problem, you will apply SVMs with different kernels to both linearly separable and non-linearly separable datasets, and perform hyperparameter tuning.\n",
    "\n",
    "## Part A: Linear vs. Non-Linear Data (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two datasets: linearly separable and non-linearly separable\n",
    "\n",
    "# Dataset 1: Linearly separable\n",
    "X_linear, y_linear = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                                        n_informative=2, n_clusters_per_class=1,\n",
    "                                        class_sep=2.0, random_state=42)\n",
    "\n",
    "# Dataset 2: Non-linearly separable (circles)\n",
    "X_nonlinear, y_nonlinear = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Split both datasets\n",
    "X_linear_train, X_linear_test, y_linear_train, y_linear_test = train_test_split(\n",
    "    X_linear, y_linear, test_size=0.3, random_state=42)\n",
    "\n",
    "X_nonlinear_train, X_nonlinear_test, y_nonlinear_train, y_nonlinear_test = train_test_split(\n",
    "    X_nonlinear, y_nonlinear, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler_linear = StandardScaler()\n",
    "X_linear_train_scaled = scaler_linear.fit_transform(X_linear_train)\n",
    "X_linear_test_scaled = scaler_linear.transform(X_linear_test)\n",
    "\n",
    "scaler_nonlinear = StandardScaler()\n",
    "X_nonlinear_train_scaled = scaler_nonlinear.fit_transform(X_nonlinear_train)\n",
    "X_nonlinear_test_scaled = scaler_nonlinear.transform(X_nonlinear_test)\n",
    "\n",
    "# Visualize the datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(X_linear_train[:, 0], X_linear_train[:, 1], c=y_linear_train, \n",
    "               cmap='RdYlBu', edgecolors='black', alpha=0.7)\n",
    "axes[0].set_title('Dataset 1: Linearly Separable')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "axes[1].scatter(X_nonlinear_train[:, 0], X_nonlinear_train[:, 1], c=y_nonlinear_train,\n",
    "               cmap='RdYlBu', edgecolors='black', alpha=0.7)\n",
    "axes[1].set_title('Dataset 2: Non-Linearly Separable (Circles)')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Datasets generated and visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE START (16 points)\n",
    "# TODO: Train SVMs with three different kernels on BOTH datasets:\n",
    "# 1. Linear kernel: SVC(kernel='linear', C=1.0)\n",
    "# 2. Polynomial kernel: SVC(kernel='poly', degree=3, C=1.0)\n",
    "# 3. RBF kernel: SVC(kernel='rbf', gamma='scale', C=1.0)\n",
    "#\n",
    "# For each kernel and each dataset:\n",
    "# - Train the SVM\n",
    "# - Make predictions on test set\n",
    "# - Compute and store the accuracy\n",
    "# - Print the number of support vectors\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "results = {'linear_data': {}, 'nonlinear_data': {}}\n",
    "\n",
    "print(\"SVM Performance with Different Kernels\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test on linearly separable data\n",
    "print(\"\\nDataset 1: Linearly Separable Data\")\n",
    "print(\"-\" * 70)\n",
    "# Your code here\n",
    "\n",
    "# Test on non-linearly separable data\n",
    "print(\"\\nDataset 2: Non-Linearly Separable Data (Circles)\")\n",
    "print(\"-\" * 70)\n",
    "# Your code here\n",
    "\n",
    "# STUDENT CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (4 points):** Compare the performance of different kernels on both datasets. Which kernel works best for the linearly separable data? Which works best for the circular data? Explain why this makes sense given the structure of the data.\n",
    "\n",
    "<!-- STUDENT ANSWER START -->\n",
    "**Your answer here:**\n",
    "\n",
    "*(Expected elements: Linear kernel should work well on linearly separable data; RBF kernel should excel on circular data; explanation of why non-linear kernels can model complex decision boundaries; discussion of when to use each kernel type)*\n",
    "<!-- STUDENT ANSWER END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Hyperparameter Tuning (20 points)\n",
    "\n",
    "Use GridSearchCV to find optimal hyperparameters for an RBF kernel SVM on the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X_cancer, y_cancer = data.data, data.target\n",
    "\n",
    "# Split and scale\n",
    "X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler_cancer = StandardScaler()\n",
    "X_cancer_train_scaled = scaler_cancer.fit_transform(X_cancer_train)\n",
    "X_cancer_test_scaled = scaler_cancer.transform(X_cancer_test)\n",
    "\n",
    "print(f\"Breast Cancer Dataset:\")\n",
    "print(f\"  Training samples: {X_cancer_train_scaled.shape[0]}\")\n",
    "print(f\"  Test samples: {X_cancer_test_scaled.shape[0]}\")\n",
    "print(f\"  Features: {X_cancer_train_scaled.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y_cancer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE START (16 points)\n",
    "# TODO: Perform hyperparameter tuning using GridSearchCV\n",
    "# 1. Define a parameter grid for C and gamma:\n",
    "#    - C: [0.1, 1, 10, 100]\n",
    "#    - gamma: ['scale', 0.001, 0.01, 0.1, 1]\n",
    "# 2. Use GridSearchCV with 5-fold cross-validation\n",
    "# 3. Fit the grid search on the training data\n",
    "# 4. Print the best parameters and best cross-validation score\n",
    "# 5. Evaluate the best model on the test set\n",
    "# 6. Print a classification report\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# STUDENT CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (4 points):** Analyze your grid search results. What were the optimal values for C and gamma? How does changing C affect the model (hint: think about the bias-variance tradeoff and the role of slack variables)? How does gamma affect the RBF kernel's decision boundary?\n",
    "\n",
    "<!-- STUDENT ANSWER START -->\n",
    "**Your answer here:**\n",
    "\n",
    "*(Expected elements: Explanation of C as regularization parameter controlling margin violations; larger C = harder margin (less tolerance for errors); gamma controls RBF kernel width; smaller gamma = smoother decision boundary; discussion of overfitting/underfitting trade-offs)*\n",
    "<!-- STUDENT ANSWER END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Comparing k-NN and SVM (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE START (7 points)\n",
    "# TODO: Compare k-NN and SVM on the breast cancer dataset\n",
    "# 1. Train a k-NN classifier (use sklearn's KNeighborsClassifier with k=5)\n",
    "# 2. Train an SVM with your best parameters from Part B\n",
    "# 3. For both models, compute:\n",
    "#    - Test accuracy\n",
    "#    - 5-fold cross-validation scores (mean and std)\n",
    "# 4. Create a comparison table\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# STUDENT CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (3 points):** Based on your comparison, which algorithm performs better on the breast cancer dataset? Consider not just accuracy, but also:\n",
    "- Computational efficiency (prediction time)\n",
    "- Number of \"parameters\" stored (all training points for k-NN vs. support vectors for SVM)\n",
    "- Interpretability\n",
    "\n",
    "In what scenarios would you prefer k-NN over SVM, and vice versa?\n",
    "\n",
    "<!-- STUDENT ANSWER START -->\n",
    "**Your answer here:**\n",
    "\n",
    "*(Expected elements: Performance comparison; k-NN stores all training data while SVM only stores support vectors; k-NN slower at prediction time; SVM better for high-dimensional data; k-NN more interpretable; discussion of when to use each)*\n",
    "<!-- STUDENT ANSWER END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Reflection\n",
    "\n",
    "**Bonus Question (5 points extra credit):** Reflect on what you've learned about k-NN and SVMs. Both are \"kernel methods\" in some sense - k-NN implicitly uses a kernel based on distance, while SVMs explicitly use kernel functions. Discuss the similarities and differences between these two perspectives. How does the notion of \"locality\" manifest in each algorithm?\n",
    "\n",
    "<!-- STUDENT ANSWER START -->\n",
    "**Your reflection (optional):**\n",
    "\n",
    "<!-- STUDENT ANSWER END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "1. **Complete all code implementations** in the sections marked `# STUDENT CODE START/END`\n",
    "2. **Answer all written questions** in the markdown cells marked `<!-- STUDENT ANSWER START/END -->`\n",
    "3. **Run all cells** to ensure your code executes without errors\n",
    "4. **Include all outputs** (plots, print statements, etc.)\n",
    "5. **Submit your completed notebook** with all code executed and outputs visible\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "**Problem 1: k-NN Implementation (50 points)**\n",
    "- Part A: Correct implementation (30 points)\n",
    "  - fit method (4 points)\n",
    "  - compute_distance for all three metrics (12 points)\n",
    "  - predict_single logic (12 points)\n",
    "  - predict method (2 points)\n",
    "- Part B: Testing and analysis (20 points)\n",
    "  - Correct testing code (13 points)\n",
    "  - Written analysis (7 points)\n",
    "\n",
    "**Problem 2: SVM with Kernels (50 points)**\n",
    "- Part A: Linear vs. non-linear data (20 points)\n",
    "  - Correct implementation (16 points)\n",
    "  - Written analysis (4 points)\n",
    "- Part B: Hyperparameter tuning (20 points)\n",
    "  - Correct grid search implementation (16 points)\n",
    "  - Written analysis (4 points)\n",
    "- Part C: Comparison (10 points)\n",
    "  - Correct comparison code (7 points)\n",
    "  - Written analysis (3 points)\n",
    "\n",
    "**Bonus (5 points extra credit)**\n",
    "\n",
    "**Total: 100 points (+ 5 bonus)**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
